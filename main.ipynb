{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-03-14T14:59:56.976684Z",
     "start_time": "2024-03-14T14:59:37.636688Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 91.0 MiB for an array with shape (23863296,) and data type float32",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mMemoryError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[2], line 12\u001B[0m\n\u001B[0;32m      9\u001B[0m ratio \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0.8\u001B[39m\n\u001B[0;32m     10\u001B[0m size \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m64\u001B[39m\n\u001B[1;32m---> 12\u001B[0m x_train, y_train, x_test, y_test, le \u001B[38;5;241m=\u001B[39m \u001B[43mpreprocessor\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpreprocess_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataset_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtraining_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtesting_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mratio\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msize\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\Program Files (x86)\\Github\\Painting-To-Artist-Classification\\preprocessor.py:37\u001B[0m, in \u001B[0;36mpreprocess_data\u001B[1;34m(dataset_path, train_path, test_path, n, ratio, size)\u001B[0m\n\u001B[0;32m     33\u001B[0m y_train_encoded, y_test_encoded, le \u001B[38;5;241m=\u001B[39m label_data(y_train, y_test)  \u001B[38;5;66;03m# Convert the labels to numbers for the model to be able to process them\u001B[39;00m\n\u001B[0;32m     35\u001B[0m x_train, x_test \u001B[38;5;241m=\u001B[39m minmax_normalize(x_train, x_test)  \u001B[38;5;66;03m# Normalize the data to be between 0 and 1 using the min-max normalization\u001B[39;00m\n\u001B[1;32m---> 37\u001B[0m x_train_features \u001B[38;5;241m=\u001B[39m \u001B[43mfeature_extraction\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx_train\u001B[49m\u001B[43m)\u001B[49m            \u001B[38;5;66;03m# Extract features from the images and align them in a dataframe\u001B[39;00m\n\u001B[0;32m     38\u001B[0m x_train_features \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mexpand_dims(x_train_features, axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m)        \u001B[38;5;66;03m# Expand the dimensions of the training dataset to be used in the model\u001B[39;00m\n\u001B[0;32m     39\u001B[0m x_train_features \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mreshape(x_train_features, (x_train\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m], \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m))  \u001B[38;5;66;03m# Reshape the training dataset to be used to train model\u001B[39;00m\n",
      "File \u001B[1;32mD:\\Program Files (x86)\\Github\\Painting-To-Artist-Classification\\preprocessor.py:343\u001B[0m, in \u001B[0;36mfeature_extraction\u001B[1;34m(dataset)\u001B[0m\n\u001B[0;32m    340\u001B[0m combined_df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mOriginal\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mconcatenate(original_images)  \u001B[38;5;66;03m# Add the original images to the dataframe\u001B[39;00m\n\u001B[0;32m    342\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m label, images_pixels \u001B[38;5;129;01min\u001B[39;00m gabor_images_dict\u001B[38;5;241m.\u001B[39mitems():\n\u001B[1;32m--> 343\u001B[0m     combined_df[label] \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mconcatenate(images_pixels)\n\u001B[0;32m    345\u001B[0m \u001B[38;5;66;03m# for label, images_pixels in sobel_images_dict.items():\u001B[39;00m\n\u001B[0;32m    346\u001B[0m \u001B[38;5;66;03m#    combined_df[label] = np.concatenate(images_pixels)\u001B[39;00m\n\u001B[0;32m    348\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m combined_df\n",
      "File \u001B[1;32mD:\\Programs\\Anaconda\\envs\\tensorFlow\\lib\\site-packages\\pandas\\core\\frame.py:4091\u001B[0m, in \u001B[0;36mDataFrame.__setitem__\u001B[1;34m(self, key, value)\u001B[0m\n\u001B[0;32m   4088\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_setitem_array([key], value)\n\u001B[0;32m   4089\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   4090\u001B[0m     \u001B[38;5;66;03m# set column\u001B[39;00m\n\u001B[1;32m-> 4091\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_set_item\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalue\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\Programs\\Anaconda\\envs\\tensorFlow\\lib\\site-packages\\pandas\\core\\frame.py:4300\u001B[0m, in \u001B[0;36mDataFrame._set_item\u001B[1;34m(self, key, value)\u001B[0m\n\u001B[0;32m   4290\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_set_item\u001B[39m(\u001B[38;5;28mself\u001B[39m, key, value) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m   4291\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   4292\u001B[0m \u001B[38;5;124;03m    Add series to DataFrame in specified column.\u001B[39;00m\n\u001B[0;32m   4293\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   4298\u001B[0m \u001B[38;5;124;03m    ensure homogeneity.\u001B[39;00m\n\u001B[0;32m   4299\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m-> 4300\u001B[0m     value, refs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_sanitize_column\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvalue\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   4302\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[0;32m   4303\u001B[0m         key \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcolumns\n\u001B[0;32m   4304\u001B[0m         \u001B[38;5;129;01mand\u001B[39;00m value\u001B[38;5;241m.\u001B[39mndim \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m   4305\u001B[0m         \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(value\u001B[38;5;241m.\u001B[39mdtype, ExtensionDtype)\n\u001B[0;32m   4306\u001B[0m     ):\n\u001B[0;32m   4307\u001B[0m         \u001B[38;5;66;03m# broadcast across multiple columns if necessary\u001B[39;00m\n\u001B[0;32m   4308\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcolumns\u001B[38;5;241m.\u001B[39mis_unique \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcolumns, MultiIndex):\n",
      "File \u001B[1;32mD:\\Programs\\Anaconda\\envs\\tensorFlow\\lib\\site-packages\\pandas\\core\\frame.py:5040\u001B[0m, in \u001B[0;36mDataFrame._sanitize_column\u001B[1;34m(self, value)\u001B[0m\n\u001B[0;32m   5038\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_list_like(value):\n\u001B[0;32m   5039\u001B[0m     com\u001B[38;5;241m.\u001B[39mrequire_length_match(value, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mindex)\n\u001B[1;32m-> 5040\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43msanitize_array\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvalue\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mindex\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcopy\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mallow_2d\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m, \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32mD:\\Programs\\Anaconda\\envs\\tensorFlow\\lib\\site-packages\\pandas\\core\\construction.py:623\u001B[0m, in \u001B[0;36msanitize_array\u001B[1;34m(data, index, dtype, copy, allow_2d)\u001B[0m\n\u001B[0;32m    620\u001B[0m         subarr \u001B[38;5;241m=\u001B[39m dtype\u001B[38;5;241m.\u001B[39mconstruct_array_type()\u001B[38;5;241m.\u001B[39m_from_sequence(data, dtype\u001B[38;5;241m=\u001B[39mdtype)\n\u001B[0;32m    622\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m subarr \u001B[38;5;129;01mis\u001B[39;00m data \u001B[38;5;129;01mand\u001B[39;00m copy:\n\u001B[1;32m--> 623\u001B[0m         subarr \u001B[38;5;241m=\u001B[39m \u001B[43msubarr\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcopy\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    625\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    626\u001B[0m     \u001B[38;5;66;03m# we will try to copy by-definition here\u001B[39;00m\n\u001B[0;32m    627\u001B[0m     subarr \u001B[38;5;241m=\u001B[39m _try_cast(data, dtype, copy)\n",
      "\u001B[1;31mMemoryError\u001B[0m: Unable to allocate 91.0 MiB for an array with shape (23863296,) and data type float32"
     ]
    }
   ],
   "source": [
    "dataset_path = os.path.abspath(r'D:\\Program Files (x86)\\painting-to-artist\\dataset')\n",
    "training_path = os.path.abspath(r'D:\\Program Files (x86)\\painting-to-artist\\workspace\\train')\n",
    "testing_path = os.path.abspath(os.path.join(r'D:\\Program Files (x86)\\painting-to-artist\\workspace\\test'))\n",
    "\n",
    "preprocessor.clear_files(training_path)\n",
    "preprocessor.clear_files(testing_path)\n",
    "\n",
    "n = 50\n",
    "ratio = 0.8\n",
    "size = 64\n",
    "\n",
    "x_train, y_train, x_test, y_test, le = preprocessor.preprocess_data(dataset_path, training_path, testing_path, n, ratio, size)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-14T15:01:23.659487Z",
     "start_time": "2024-03-14T15:00:01.812212Z"
    }
   },
   "id": "4252593350c50b62",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.65\n"
     ]
    }
   ],
   "source": [
    "RF_model = RandomForestClassifier(n_estimators=50, random_state=0)\n",
    "RF_model.fit(x_train, y_train)\n",
    "\n",
    "test_prediction = RF_model.predict(x_test)\n",
    "\n",
    "print(\"Accuracy:\", metrics.accuracy_score(y_test, test_prediction))\n",
    "\n",
    "test_prediction = le.inverse_transform(test_prediction)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-14T14:54:42.897534Z",
     "start_time": "2024-03-14T14:54:31.287581Z"
    }
   },
   "id": "c1cc855f1bb6b660",
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "4d5625e763646923"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
